---
title: GCN
author: Mingkai Qiu
date: 2023-09-19
category: content
layout: post
---

# GCN

## GNN
> 本文部分内容参考自：[传送门](http://xtf615.com/2019/02/24/gcn/)


图网络可以理解成给定网络的拓扑结构，包括点和边，输入点和边的特征（边可能没有特征，只有一个权重），得到关于每一个点的输出作为点的预测，而损失，就是定义为点的预测值和实际值的一个比如交叉熵损失或者是MSE。

上面的解释是一种专注于节点的图（node-focused），如果是专注于图（graph-focused），则是在图中新增加一个节点，表示一个全图的节点，然后只预测这个点的值与图的实际值进行一个损失计算。

图中涉及到的参数定义如下,在图中，一般用$l$表示特征向量，而$x$表示状态向量

![参数定义](/images/GCN-notation.png "GCN-notation")

**节点更新**  GNN中，点之间通过交换信息来更新自己的节点状态，也就是节点的状态根据点自身的特征，邻居节点的特征来进行更新，直到达到一个稳定值。

因此，参数更新的过程，就是对图网络的特征进行更新后，不断迭代到稳定状态后，再计算损失，进行参数更新。

对于一个节点，其状态向量$x_n\in R^s$以及输出$o_n$的更新方式为：

$x_n = f_A(l_n,l_{co[n]},x_{ne[n]},l_{ne[n]})$

$o_n = g_A(x_n, l_n)$
也就是，点的状态向量与自身特征，连接边的特征，连接点的特征以及状态向量相关（此处假设边没有状态向量，有特征向量，如果没有特征向量，则只有权重，体现在邻接矩阵中），然后再根据点的状态向量，得到输出结果。

节点更新及模型优化步骤如下：

1. 聚合邻居信息：对于每一个节点，它会从其邻居节点收集信息。这通常是通过一种称为“聚合函数”的函数来实现的。常见的聚合函数包括均值、和、最大值或者concat等；
2. 更新节点表示：根据收集到的邻居信息以及当前节点的信息，更新该节点的表示。这通常是通过一种称为“更新函数”的函数来实现的。一个常见的更新函数是将聚合得到的邻居信息与当前节点信息连接起来，并通过一个全连接层或其他非线性激活函数进行处理。
3. 重复迭代：重复上面的过程中，迭代T次后得到接近不动解：$x(T) \approx x$， $T$一般是一个超参数
4. 梯度计算：计算参数权重的梯度
5. 权重更新：使用该梯度更新权重

**具体实现**
对于上面状态向量的更新，在编程时可以表示为如下：

$x_n = \sum_{u\in ne[n]}h_A(l_n, l_{(n,u)},x_u,l_u), n\in N$

这里也就是把点的特征向量、边的特征向量、邻居节点的特征向量和状态向量concat起来（或者是均值、和、最大值等），然后输入一个MLP（也就是$h_A$函数）中就可以得到更新后的状态向量

## GCN

**GCN与拉普拉斯矩阵** 图卷积网络（GCN）是一种深度学习模型，用于处理图数据。为了在图上执行卷积，GCN利用了图的拉普拉斯矩阵（Laplacian Matrix）的特性。拉普拉斯矩阵描述了图中节点之间的关系，是许多图论算法和图信号处理方法的基础。

给定一个无向图 $G = (V, E)$ ，其中 $V$ 是节点集， $E$ 是边集，我们可以定义图的邻接矩阵 $A$ 和度矩阵 $D$。

1. **邻接矩阵 $A$**: 
   - $A$ 是一个 $V\times V$ 的矩阵。
   - 如果节点 $i$ 和节点 $j$ 之间存在边，则 $A_{ij} = 1$；否则 $A_{ij} = 0$。
  
2. **度矩阵 $D$**:
   - $D$ 是一个对角矩阵，其中对角线上的元素 $D_{ii}$ 表示节点 $i$ 的度（与它相连的边的数量）。

拉普拉斯矩阵 $L$ 定义为：$L = D - A$

GCN 中经常使用的是归一化的拉普拉斯矩阵，定义为：
$\hat{L} = I - D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$
其中 $I$ 是单位矩阵。

该归一化有助于使得传播过程更为稳定。在 GCN 中，特征传播过程可以简化为应用归一化的邻接矩阵到节点特征上。


拉普拉斯矩阵是一个半正定对称矩阵，包含如下性质：
- 对角线上的元素代表每个节点的度。
- 一定具有n个线性无关的特征向量（n是Graph节点数）
- 半正定矩阵特征值一定非负
- 对称矩阵特征向量相互正交，即所有特征向量构成的矩阵为正交矩阵

在图卷积网络（GCN）中，拉普拉斯矩阵是关键的组成部分，因为它描述了图中节点之间的连接关系，这有助于特征在图上的传播。

### GCN1代

传统的傅里叶变换如下：

![傅里叶变换](/images/GCN-fourier.png "GCN-fourier")

在GCN上，利用拉普拉斯矩阵，设$\Phi_k$是拉普拉斯矩阵的特征向量，$\lambda_k$是特征值，在GCN中，与传统傅里叶变换做了下面的对应：

![傅里叶变换对应](/images/GCN-fourier1.png "GCN-fourier1")

此时图上的傅里叶变换的矩阵形式则变成$\hat{f} = \Phi^{T}f$。因为$\Phi$是拉普拉斯矩阵的特征向量矩阵，这个矩阵是一个正交矩阵，也就是逆等于转置($\Phi^T \Phi = I$)，所以有$f = \Phi \hat{f}$，也就是逆变换时，直接左乘$\Phi$即可

考虑卷积定理：函数卷积的傅里叶变换是函数傅里叶变换的乘积公式如下：

$f*g = F^{-1}\{F\{f\}\cdot F\{g\}\}$

也就是时域上的卷积等于先傅里叶变换转化为频域后进行乘积，再逆傅里叶变换的结果。
则此时在图上的卷积就变成：$(f*h)_{g} = \Phi diag[\hat{h}(\lambda_1),\dots,\hat{h}(\lambda_1)]\Phi^Tf$,这里左乘$\Phi$就是傅里叶逆变换，第二个的diag则是为了进行对应位置元素的乘积，将卷积核$h$的傅里叶变换$\Phi^T h\in R^{N\times 1}$（图的卷积核就是一维，大小为图节点数目的卷积核）转换成对角矩阵的形式（这里也是将特征向量表示为特征值的函数），后面两个则是函数$f$的傅里叶变换。

这里把卷积核的傅里叶变换转换成对角矩阵的形式，是因为这样就是把一维卷积核的傅里叶变换，直接变成二维卷积核，也就是把傅里叶变换后的结果直接作为参数向量，让网络进行学习。

这个表达形式就是第一代的GCN，这里存在的问题是：

- 复杂度太高，需要对拉普拉斯矩阵进行谱分解求解特征向量$\Phi$，这个在图很大的时候复杂度较高，为$O(n^2)$

- 卷积核的参数为n，当图较大的时候，卷积核很大

### GCN2代

卷积核$diag[\hat{h}(\lambda_1),\dots,\hat{h}(\lambda_1)]$是关于特征值的函数，可以写成$g_{\theta}(\Lambda)$，然后把这个定义成k阶多项式的形式：$g_{\theta}(\Lambda)\approx \sum_{k=0}^K{\theta_k \Lambda^k}$,则原始的卷积操作可以变成：

$g_{\theta}*x\approx \Phi \sum_{k=0}^K \theta_{k} \Lambda^k\Phi^T x$
    
$=\sum_{k=0}^K \theta_{k}(\Phi \Lambda^k \Phi^T)x$

$=\sum_{k=0}^K \theta_{k}(\Phi \Lambda \Phi^T)^kx$

=$=\sum_{k=0}^K\theta_kL^k x$

这里就不需要进行特征分解，只需要对拉普拉斯矩阵进行变换，实现将$L^k$计算出来，则可以在前行传播的时候使用，复杂度为$O(Kn^2)$。

同时，由于矩阵的k次方可以求连通性，也就是假设给定一个矩阵表示邻接关系，则矩阵的k次方后，非零元素就表示对应的$i,j$点在k次内可达，也就是$L$矩阵的$k$次方的含义就表示了$k-hop$之内的节点，也就是$k$次方使得节点的更新只用到了节点的$K$-Localized信息。

同时，考虑到**任何多项式都可以使用切比雪夫展开式近似（类比函数可以利用泰勒展开进行近似）**，则可以引入切比雪夫多项式的$K$阶截断来获得对$L^K$的近似，进而获得对$g_{\theta}(\Lambda)的近似$：

$$g_{\theta^{\prime}}(\Lambda) \approx\sum_{k=0}^K{\theta^{\prime}_{k}T_{k}(\tilde{\Lambda})}$$

其中$\tilde{\Lambda} = \frac{2}{\lambda_{max}}\Lambda - I_n$，是拉普拉斯矩阵$L$的最大值(即谱半径)缩放后的特征向量矩阵（也就是最大值缩放为1，防止连乘爆炸），$\theta^{\prime} \in R^{K}$是一个切比雪夫向量，$\theta^{\prime}_{k}$是第k维分量。

则可以得到：$$g_{\theta}*x\approx \sum_{k=0}^K \theta_{k}^{\prime}(\Phi T_K(\tilde{\Lambda})\Phi^T)^kx = \sum_{k=0}^K \theta^{\prime}_{k}T_{k}(\tilde{L})x$$

其中$\tilde{L} = \frac{2}{\lambda_{max}}L-I_n$.

因为切比雪夫多项式可以用递归来定义:$T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x), T_{0}(x) = 1, T_1(x) = x$，因此卷积的计算就变成可以迭代计算的公式，用于替换原始矩阵的k次方计算。

### GCN3代
GCN3代在2代的基础上，进行了进一步简化：

- 取K=1，此时模型是1阶近似，即每层卷积层只考虑了直接邻域，相当于CNN中$3*3$的卷积核；
- 深度加深，宽度减少，如果要建立多阶近似，只需要使用多个层即可；
- 加入参数的约束，如: $\lambda_{max}\approx2$。

当$\lambda_{max}=2， K=1$时，原始公式变成：$g_{\theta}^{\prime}*x\approx \theta_0^{\prime}x + \theta_1^{\prime}\tilde{L}x = \theta_0^{\prime}x + \theta_1^{\prime}(L-I_n)x = \theta_0^{\prime}x - \theta_1^{\prime}(D^{-1/2}AD^{-1/2})x$，这里是利用了归一化拉普拉斯矩阵$L = D^{-1/2}(D-A)D^{-1/2} = I_n - D^{-1/2}AD^{-1/2}$，$A$是邻接矩阵，此时就只有两个参数了。

进一步假设$$\theta^{\prime}_0 = -\theta^{\prime}_1$$，则此时只有一个参数$$\theta$$: $$g_{\theta}^{\prime}*x = \theta(I_n+D^{-1/2}AD^{-1/2})x$$。


同时，考虑到$I_n+D^{-1/2}AD^{-1/2}$的谱半径（也就是最大特征值）在[0,2]太大，所以进行重归一化，变成:$\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$，其中$\tilde{A} = A+I_n$，也就是邻接矩阵加上自连接，$$\tilde{D}_{i,j} = \sum_{j} \tilde{A}_{i,j}$$，也就是变换后的连接矩阵的度矩阵（点的度为所连接的点的数目）。

那么最后的形式就是：$g_{\theta}^{\prime}*x = \theta(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2})x$。

考虑多通道多卷积核（也就是$x\in R^{N\times 1}$变成$X\in R^{N\times C}$,卷积核从$\theta\in R$变成$\theta\in R^{C\times F}$，$C$为通道数，$F$为卷积核数量）。

写成矩阵形式变成：$Z = \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}X\Theta$ (其中$\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}\in R^{N\times N}, X\in R^{N\times C}, \Theta\in R^{C\times F}$,最后输出$Z\in R^{N\times F}$,也就是卷积结果的维数等于卷积核的数量)。




## GCN的数学形式
图卷积网络（GCN）的数学形式旨在将节点的特征和图的结构信息结合起来，以获得每个节点的新的特征表示。以下是GCN的基本数学形式：

考虑一个图 $G$，其邻接矩阵为 $A$，并附加一个单位矩阵 $I$ 以考虑自环。因此，修正后的邻接矩阵为 $\tilde{A} = A + I$。

对于每个节点，我们还有一个特征矩阵 $X$，其中第 $i$ 行是节点 $i$ 的特征向量。

1. **计算度矩阵**:
首先，我们计算图 $G$ 的度矩阵 $\tilde{D}$，其中 $\tilde{D}_{ii}$ 是节点 $i$ 的度（与它相连的边的数量）。

2. **特征传播**:
GCN的关键步骤是特征传播，这是通过以下公式实现的：
$H^{(l+1)} = \sigma\left( \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} A^{(l)} \right)$
其中：
- $H^{(l)}$ 是第 $l$ 层的节点特征矩阵。
- $H^{(0)}$ 是初始的节点特征矩阵，即 $X$。
- $A^{(l)}$ 是第 $l$ 层的权重矩阵，需要在训练过程中学习。
- $\sigma$ 是一个非线性激活函数，例如ReLU。

3. **多层传播**:
实际的GCN模型可能包含多个这样的层，每一层都通过上述公式进行特征传播。这意味着每个节点在每一层都会聚合其邻居的信息。

4. **输出**:
对于节点分类任务，最后一层的输出 $H^{(L)}$ 可以传递给一个分类器进行处理。对于整个图的分类或回归任务，可以使用全局池化策略（如平均、最大池化等）来从最后一层的所有节点特征中获取单个图的表示。

GCN的特征传播公式可以分为以下几个步骤进行理解：

-    **邻接矩阵的归一化**:
$\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$
这一步确保了特征在邻居之间的传播是稳定的，特别是在聚合多个邻居的信息时。归一化后的邻接矩阵能够使得每个节点的贡献与其度数成反比。

-    **特征聚合**:
乘以 $H^{(l)}$ 是特征的聚合步骤，它考虑了每个节点及其邻居的特征。简单地说，这一步骤将一个节点及其邻居的特征聚合到该节点上。

-    **线性转换**:
最后，乘以权重矩阵 $A^{(l)}$ 是一个线性转换，它可以将聚合后的特征映射到新的特征空间。这允许模型学习更复杂和抽象的特征表示。

-    **非线性激活**:
函数 $\sigma$ 是一个非线性激活函数（如ReLU），它为模型引入了非线性，使模型能够学习更复杂的函数。


## GCN的归一化
可以看到GCN中使用的归一化为$\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$，而一般的拉普拉斯矩阵归一化为：$I - D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$
这两种表示实际上源于相同的思想，但在实践中有些差异。这两种形式都试图通过归一化来稳定特征的传播，但它们的应用和解释略有不同。

1. **谱域的解释**:
    归一化的拉普拉斯矩阵 $\hat{L} = I - D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$ 起源于图信号处理的谱域方法。在谱图卷积的背景下，这种归一化有助于确保特征值位于\[0, 2\]的范围内，从而使得谱滤波变得更加稳定。

2. **空间域的解释**:
    在GCN的原始论文中，作者采用了空间域的方法，而不是谱域的方法。他们的目标是直接模拟局部邻域内的信息聚合。为了实现这一点，他们使用了修正的邻接矩阵 $\tilde{A} = A + I$（加入自环）和相应的度矩阵 $\tilde{D}$。然后，他们使用了归一化的形式 $\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$ 来确保信息在邻居之间传播时的稳定性。

基于空间域的解释中，归一化的目的是确保节点特征的传播稳定并防止梯度消失或爆炸。我们可以从直观和数学两个方面来理解这种归一化的效果。

1. **直观解释**:
   - 当我们考虑一个节点及其邻居时，我们希望这些节点的贡献是均衡的，而不是由于某些节点的度数特别高而被主导。换句话说，一个与许多其他节点相连的节点不应该对聚合结果产生过大的影响。
   - 通过归一化，我们确保每个节点的贡献与其度数成反比。这意味着度数高的节点在聚合过程中的权重会减少，而度数低的节点的权重会增加，从而使得聚合过程更加均衡。

2. **数学解释**:
   - 考虑邻接矩阵 $A$ 的一个简单操作，即与特征矩阵 $H$ 的矩阵乘法，即 $AH$。这实际上是将每个节点的特征与其邻居相加。但是，如果一个节点有很多邻居，那么它的特征值可能会变得非常大。
   - 为了解决这个问题，我们使用度矩阵 $D$ 进行归一化。具体来说，我们使用 $D^{-\frac{1}{2}}$ 来按节点的度数对每个节点进行归一化。
   - 因此，操作 $\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H$ 实际上是首先使用 $\tilde{D}^{-\frac{1}{2}}$ 对每个节点的特征进行归一化，然后进行邻居聚合，最后再次归一化。

通过这种方式，归一化确保了特征传播的稳定性，无论节点的度数如何。这种稳定性是训练深层神经网络所必需的，因为它可以防止梯度消失或爆炸，从而使训练过程更加稳定。

两种方法的核心思想是相似的：通过适当的归一化确保稳定的特征传播。但由于GCN原始论文中的方法更加直观并且易于理解（考虑到它是基于空间域的），因此它已经成为了这一领域的标准。

